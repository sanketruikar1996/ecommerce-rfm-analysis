{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c3dd5eb-3676-4aa3-a065-2560bdd15d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1) Initialize Spark session\n",
    "# SparkSession is the entry point for working with Spark DataFrames\n",
    "spark = SparkSession.builder.appName(\"ECommerceData\").getOrCreate()\n",
    "\n",
    "# 2) Load raw CSV dataset\n",
    "# header=True ensures column names are inferred from first row\n",
    "# inferSchema=True automatically detects column data types\n",
    "df_raw = spark.read.csv(\n",
    "    \"/Volumes/workspace/default/online_retail_dataset/online_retail.csv\", \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# 3) Inspect schema and first few rows to understand data structure\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5)\n",
    "\n",
    "# 4) Save raw dataset as Parquet for faster future access\n",
    "# Parquet is an efficient columnar storage format widely used in industry\n",
    "df_raw.write.mode(\"overwrite\").parquet(\n",
    "    \"/Volumes/workspace/default/online_retail_dataset/online_retail_raw.parquet\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01485a1c-1074-43e9-86db-7771ed6a9baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7761d7a5-5896-4947-b3e6-043e5ad1ce38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "536641"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate records to ensure data integrity\n",
    "# This prevents double-counting in subsequent analyses\n",
    "df_clean = df_raw.dropDuplicates()\n",
    "df_clean.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f91aab-81c6-4495-9804-16947a93fa32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|        0|        0|       1454|       0|          0|        0|    135037|      0|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verify remaining missing values per column\n",
    "# This helps ensure that all critical columns are complete before further analysis\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df_clean.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_clean.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3623f56-f0a9-4131-a183-c66f3c72280f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n|        0|        0|          0|       0|          0|        0|         0|      0|\n+---------+---------+-----------+--------+-----------+---------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "## Handle Missing Values\n",
    "# 1) Fill missing product descriptions with \"Unknown\" to retain the row while marking incomplete data\n",
    "df_clean = df_clean.fillna({\"Description\": \"Unknown\"})\n",
    "\n",
    "# 2) Drop rows without CustomerID since these cannot be linked to any customer for analysis\n",
    "df_clean = df_clean.na.drop(subset=[\"CustomerID\"])\n",
    "\n",
    "# 3) Verify remaining missing values per column after handling\n",
    "df_clean.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_clean.columns]).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc81f72f-9834-4eb1-af81-c0abc521c395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "401604"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582218ae-167b-439f-b0be-2ea297250d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n|Missing_Quantity|Missing_UnitPrice|\n+----------------+-----------------+\n|               0|                0|\n+----------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Inspect missing (null) values in Quantity and UnitPrice columns\n",
    "# This helps identify incomplete transactions that may need handling before analysis\n",
    "from pyspark.sql.functions import col,count,when\n",
    "\n",
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "df_clean.select(\n",
    "    count(when(col(\"Quantity\").isNull(), True)).alias(\"Missing_Quantity\"),\n",
    "    count(when(col(\"UnitPrice\").isNull(), True)).alias(\"Missing_UnitPrice\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6a7068-443a-4daf-9cda-d2d7c341fb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n|Negative_Quantity|Negative_UnitPrice|\n+-----------------+------------------+\n|             8872|                 0|\n+-----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Check for negative values in Quantity and UnitPrice columns \n",
    "# to identify any invalid transactions before cleaning the data\n",
    "\n",
    "from pyspark.sql.functions import count,col,when\n",
    "\n",
    "df_clean.select(\n",
    "    count(when(col(\"Quantity\") < 0, True)).alias(\"Negative_Quantity\"),\n",
    "    count(when(col(\"UnitPrice\") < 0, True)).alias(\"Negative_UnitPrice\")\n",
    ").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10671325-ad40-4fc2-8132-f1cd74ec5760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "392732"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with negative Quantity values to ensure data integrity, \n",
    "# as negative quantities are invalid for sales transaction analysis\n",
    "from pyspark.sql.functions import col\n",
    "df_clean=df_clean.filter(col(\"Quantity\")>=0)\n",
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7bde23d-c44a-4445-9092-5ca59e870dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = false)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- Country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Convert CustomerID to string type to ensure consistent data type for grouping and analysis,\n",
    "# which is important for accurate aggregation and downstream processing\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df_clean=df_clean.withColumn(\"CustomerID\",col(\"CustomerID\").cast(StringType()))\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12913df-f3dd-496b-9f1a-f854f0b12fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Count : 0\n"
     ]
    }
   ],
   "source": [
    "## Duplicate Count\n",
    "duplicate_count = df_clean.count() - df_clean.dropDuplicates().count()\n",
    "print(f\"Duplicate Count : {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb4c63c-06e9-4fac-a916-bf87e2ebd6f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------------+\n|CustomerID|Total_Transactions_Per_Customer|\n+----------+-------------------------------+\n|   14165.0|             120.44000000000003|\n|   17376.0|                          203.2|\n|   17928.0|                         212.54|\n|   16527.0|                         228.06|\n|   13721.0|                         524.14|\n+----------+-------------------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 1) Aggregated total transaction value per customer by summing Quantity * UnitPrice,\n",
    "# providing insights into customer-level revenue contribution and enabling customer segmentation analysis\n",
    "\n",
    "from pyspark.sql.functions import sum,col\n",
    "total_transactions = df_clean.groupBy(\"CustomerID\")\\\n",
    "                     .agg(sum(col(\"Quantity\")* col(\"UnitPrice\")).alias(\"Total_Transactions_Per_Customer\"))\n",
    "total_transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23db7266-723e-4e89-bc4b-83a21e985ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|total_transactions|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+\n|   536615|    22634|CHILDS BREAKFAST ...|       2|2010-12-02 10:09:00|     9.95|   14047.0|United Kingdom|              19.9|\n|   536638|    22297|HEART IVORY TRELL...|      24|2010-12-02 11:41:00|     1.25|   16244.0|United Kingdom|              30.0|\n|   536745|    22737|RIBBON REEL CHRIS...|      10|2010-12-02 13:38:00|     1.65|   17235.0|United Kingdom|              16.5|\n|   536412|   85049E|SCANDINAVIAN REDS...|       3|2010-12-01 11:49:00|     1.25|   17920.0|United Kingdom|              3.75|\n|   536415|    22594|CHRISTMAS GINGHAM...|       5|2010-12-01 11:57:00|     0.85|   12838.0|United Kingdom|              4.25|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2) Calculated total transaction value for each invoice by multiplying UnitPrice with Quantity,\n",
    "# enabling analysis of individual transaction contributions to overall revenue\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "total_transactions_value= df_clean.withColumn(\"total_transactions\",col(\"UnitPrice\") * col (\"Quantity\"))\n",
    "total_transactions_value.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fbfb63-a711-41f7-b54f-515596dc4562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n|CustomerID|avg(total_transactions)|\n+----------+-----------------------+\n|   14165.0|      5.018333333333334|\n|   17376.0|      8.466666666666667|\n|   17928.0|      9.240869565217391|\n|   16527.0|                  16.29|\n|   13721.0|     18.719285714285714|\n+----------+-----------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 3) Calculated average transaction value per customer to assess customer purchasing behavior and support segmentation strategies\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avg_transactional_value=total_transactions_value.groupBy(\"CustomerID\").avg(\"total_transactions\").alias(\"avg_transactions\")\n",
    "avg_transactional_value.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f3e55d-2836-4f0f-b0e9-2e642f9696bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------+\n|       Country|Total_Revenue_Per_Country|\n+--------------+-------------------------+\n|United Kingdom|        7285024.643999968|\n|   Netherlands|                285446.34|\n|          EIRE|       265262.46000000014|\n|       Germany|        228678.3999999999|\n|        France|       208934.30999999985|\n+--------------+-------------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 4) Aggregated transactional sales data to calculate total revenue per country, providing insights for regional performance analysis\n",
    "from pyspark.sql.functions import sum,col\n",
    "\n",
    "total_revenue_per_country=df_clean.groupBy(\"Country\")\\\n",
    "    .agg(sum(col(\"UnitPrice\")* col(\"Quantity\")).alias(\"Total_Revenue_Per_Country\"))\n",
    "total_revenue_per_country.orderBy(col(\"Total_Revenue_Per_Country\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9621305d-4379-4c48-ba11-77a7a0cb5d2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n|CustomerID|   LastPurchaseDate|\n+----------+-------------------+\n|   13502.0|2011-11-23 10:39:00|\n|   13064.0|2011-11-27 12:31:00|\n|   16554.0|2011-10-28 14:51:00|\n|   15160.0|2010-12-17 14:15:00|\n|   13225.0|2011-12-06 13:29:00|\n+----------+-------------------+\nonly showing top 5 rows\n+----------+-------------------+-------+\n|CustomerID|   LastPurchaseDate|Recency|\n+----------+-------------------+-------+\n|   13502.0|2011-11-23 10:39:00|     16|\n|   13064.0|2011-11-27 12:31:00|     12|\n|   16554.0|2011-10-28 14:51:00|     42|\n|   15160.0|2010-12-17 14:15:00|    357|\n|   13225.0|2011-12-06 13:29:00|      3|\n+----------+-------------------+-------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 1) Calculating Recency for each customer:\n",
    "# Recency measures the number of days since a customer's last purchase.\n",
    "# This metric is critical in industry to identify active vs. inactive customers.\n",
    "# Lower recency indicates more recent engagement, which can inform retention campaigns\n",
    "# and targeted marketing strategies.\n",
    "from pyspark.sql.functions import max,col,datediff, lit\n",
    "# Step 1: Find the latest purchase date in the dataset\n",
    "max_date= df_clean.select(max(\"InvoiceDate\")).collect()[0][0]\n",
    "max_date\n",
    "# Step 2(a): Find the most recent purchase date for each customer\n",
    "last_purchase_df = df_clean.groupBy(\"CustomerID\").agg(max(\"InvoiceDate\").alias(\"LastPurchaseDate\"))\n",
    "last_purchase_df.show(5)\n",
    "# Step 2()b:Calculate Recency: days since last purchase\n",
    "recency_df =last_purchase_df.withColumn(\"Recency\",datediff(lit(max_date),col(\"LastPurchaseDate\")))\n",
    "recency_df .show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82f21058-34a0-4a35-b264-d66aa437fbd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|CustomerID|Frequency|\n+----------+---------+\n|   17841.0|     7676|\n|   14911.0|     5672|\n|   14096.0|     5111|\n|   12748.0|     4413|\n|   14606.0|     2677|\n+----------+---------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2) Aggregated customer purchase data to calculate transaction frequency per customer, \n",
    "# providing insights into customer loyalty and engagement patterns for targeted retention strategies.\n",
    "from pyspark.sql.functions import count, col\n",
    "frequency_df = df_clean.groupBy(\"CustomerID\") \\\n",
    "                       .agg(count(\"InvoiceNo\").alias(\"Frequency\"))\n",
    "# Show top 5 customers by frequency\n",
    "frequency_df.orderBy(col(\"Frequency\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c258b08f-98fd-4a3c-9765-8d21fc0e85b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------+\n|CustomerID|total_revenue_per_customer|\n+----------+--------------------------+\n|   14165.0|        120.44000000000003|\n|   17376.0|                     203.2|\n|   17928.0|                    212.54|\n|   16527.0|                    228.06|\n|   13721.0|                    524.14|\n+----------+--------------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 3) Calculated Monetary Value (total revenue generated by each customer).\n",
    "# This metric is widely used in industry to measure customer lifetime value,\n",
    "# helping businesses identify high-revenue customers and prioritize retention/upselling strategies.\n",
    "\n",
    "from pyspark.sql.functions import sum,col\n",
    "\n",
    "monetary_df=df_clean.groupBy(\"CustomerID\")\\\n",
    "    .agg(sum(col(\"Quantity\") * col(\"UnitPrice\")).alias(\"total_revenue_per_customer\"))\n",
    "monetary_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0a9ff5-c826-43ff-a28a-c1f6044ed292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------+---------+--------------------------+\n|CustomerID|   LastPurchaseDate|Recency|Frequency|total_revenue_per_customer|\n+----------+-------------------+-------+---------+--------------------------+\n|   14165.0|2011-03-09 12:39:00|    275|       24|        120.44000000000003|\n|   17376.0|2011-09-30 12:36:00|     70|       24|                     203.2|\n|   17928.0|2011-10-25 13:52:00|     45|       23|                    212.54|\n|   16527.0|2011-09-19 16:21:00|     81|       14|                    228.06|\n|   13721.0|2011-11-03 14:05:00|     36|       28|                    524.14|\n+----------+-------------------+-------+---------+--------------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Combine R, F, M into one dataframe\n",
    "\n",
    "rfm_df=recency_df.join(frequency_df,on=\"CustomerID\",how=\"inner\")\\\n",
    "    .join(monetary_df,on=\"CustomerID\",how=\"inner\")\n",
    "rfm_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5128fca3-59fe-43bd-8400-4f47ae3bf7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------+---------+--------------------------+-------+\n|CustomerID|   LastPurchaseDate|Recency|Frequency|total_revenue_per_customer|R_Score|\n+----------+-------------------+-------+---------+--------------------------+-------+\n|   17908.0|2010-12-01 11:45:00|    373|       54|        232.02999999999997|      1|\n|   16048.0|2010-12-01 15:28:00|    373|        8|        256.44000000000005|      1|\n|   17968.0|2010-12-01 12:23:00|    373|       81|        265.09999999999997|      1|\n|   14729.0|2010-12-01 12:43:00|    373|       71|                    313.49|      1|\n|   18011.0|2010-12-01 17:35:00|    373|       28|                    102.79|      1|\n+----------+-------------------+-------+---------+--------------------------+-------+\nonly showing top 5 rows\n+----------+-------------------+-------+---------+--------------------------+-------+-------+\n|CustomerID|   LastPurchaseDate|Recency|Frequency|total_revenue_per_customer|R_Score|F_Score|\n+----------+-------------------+-------+---------+--------------------------+-------+-------+\n|   13747.0|2010-12-01 10:37:00|    373|        1|                      79.6|      1|      1|\n|   15070.0|2010-12-02 11:23:00|    372|        1|                     106.2|      1|      1|\n|   15823.0|2010-12-02 15:08:00|    372|        1|                      15.0|      1|      1|\n|   14576.0|2010-12-02 17:51:00|    372|        1|        35.400000000000006|      1|      1|\n|   17925.0|2010-12-02 10:10:00|    372|        1|                    244.08|      1|      1|\n+----------+-------------------+-------+---------+--------------------------+-------+-------+\nonly showing top 5 rows\n+----------+-------------------+-------+---------+--------------------------+-------+-------+-------+\n|CustomerID|   LastPurchaseDate|Recency|Frequency|total_revenue_per_customer|R_Score|F_Score|M_Score|\n+----------+-------------------+-------+---------+--------------------------+-------+-------+-------+\n|   13256.0|2011-11-25 15:57:00|     14|        1|                       0.0|      4|      1|      1|\n|   16738.0|2011-02-15 09:46:00|    297|        1|                      3.75|      1|      1|      1|\n|   14792.0|2011-10-07 09:19:00|     63|        2|                       6.2|      3|      1|      1|\n|   16454.0|2011-10-26 11:40:00|     44|        2|                       6.9|      3|      1|      1|\n|   17956.0|2011-04-04 13:47:00|    249|        1|                     12.75|      1|      1|      1|\n+----------+-------------------+-------+---------+--------------------------+-------+-------+-------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import ntile, col\n",
    "\n",
    "recency_window=Window.orderBy(col(\"Recency\").desc())\n",
    "rfm_df =rfm_df.withColumn(\"R_Score\",ntile(5).over(recency_window))\n",
    "rfm_df.show(5)\n",
    "\n",
    "\n",
    "frequency_window=Window.orderBy(col(\"Frequency\").asc())\n",
    "rfm_df =rfm_df.withColumn(\"F_Score\",ntile(5).over(frequency_window))\n",
    "rfm_df.show(5) \n",
    "\n",
    "monetary_window=Window.orderBy(col(\"total_revenue_per_customer\").asc())\n",
    "rfm_df =rfm_df.withColumn(\"M_Score\",ntile(5).over(monetary_window))\n",
    "rfm_df.show(5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9070dddb-cbc2-4fe6-a0b5-445001d738f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------------------------+-------+-------+-------+-----------+\n|CustomerID|Recency|Frequency|total_revenue_per_customer|R_Score|F_Score|M_Score|RFM_Segment|\n+----------+-------+---------+--------------------------+-------+-------+-------+-----------+\n|   13256.0|     14|        1|                       0.0|      4|      1|      1|      4-1-1|\n|   16738.0|    297|        1|                      3.75|      1|      1|      1|      1-1-1|\n|   14792.0|     63|        2|                       6.2|      3|      1|      1|      3-1-1|\n|   16454.0|     44|        2|                       6.9|      3|      1|      1|      3-1-1|\n|   17956.0|    249|        1|                     12.75|      1|      1|      1|      1-1-1|\n|   16878.0|     84|        3|                      13.3|      2|      1|      1|      2-1-1|\n|   15823.0|    372|        1|                      15.0|      1|      1|      1|      1-1-1|\n|   17763.0|    263|        1|                      15.0|      1|      1|      1|      1-1-1|\n|   13307.0|    120|        1|                      15.0|      2|      1|      1|      2-1-1|\n|   16093.0|    106|        1|                      17.0|      2|      1|      1|      2-1-1|\n+----------+-------+---------+--------------------------+-------+-------+-------+-----------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Combine into a segment string\n",
    "from pyspark.sql.functions import concat_ws\n",
    "rfm_df = rfm_df.withColumn(\"RFM_Segment\", concat_ws(\"-\", col(\"R_Score\"), col(\"F_Score\"), col(\"M_Score\")))\n",
    "\n",
    "rfm_df.select(\"CustomerID\", \"Recency\", \"Frequency\", \"total_revenue_per_customer\",\n",
    "              \"R_Score\", \"F_Score\", \"M_Score\", \"RFM_Segment\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baec5f96-b394-4bb6-8d66-c299ad00b687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+-----------+---------+\n|CustomerID|R_Score|F_Score|M_Score|RFM_Segment|RFM_Score|\n+----------+-------+-------+-------+-----------+---------+\n|13755.0   |5      |5      |5      |5-5-5      |5.0      |\n|13126.0   |5      |5      |5      |5-5-5      |5.0      |\n|16407.0   |5      |5      |5      |5-5-5      |5.0      |\n|12856.0   |5      |5      |5      |5-5-5      |5.0      |\n|17861.0   |5      |5      |5      |5-5-5      |5.0      |\n|13571.0   |5      |5      |5      |5-5-5      |5.0      |\n|12518.0   |5      |5      |5      |5-5-5      |5.0      |\n|14696.0   |5      |5      |5      |5-5-5      |5.0      |\n|18044.0   |5      |5      |5      |5-5-5      |5.0      |\n|12935.0   |5      |5      |5      |5-5-5      |5.0      |\n+----------+-------+-------+-------+-----------+---------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Define weights (industry often prioritizes Recency a bit higher)\n",
    "weight_R = 0.5\n",
    "weight_F = 0.3\n",
    "weight_M = 0.2\n",
    "\n",
    "# Calculate weighted RFM score\n",
    "rfm_df = rfm_df.withColumn(\n",
    "    \"RFM_Score\",\n",
    "    (col(\"R_Score\") * weight_R) +\n",
    "    (col(\"F_Score\") * weight_F) +\n",
    "    (col(\"M_Score\") * weight_M)\n",
    ")\n",
    "\n",
    "# Show top 10 customers by RFM_Score\n",
    "rfm_df.select(\"CustomerID\", \"R_Score\", \"F_Score\", \"M_Score\", \"RFM_Segment\", \"RFM_Score\") \\\n",
    "      .orderBy(col(\"RFM_Score\").desc()) \\\n",
    "      .show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e35a9d63-7e3d-4f70-9e7c-45af176c5abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1061: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|RFM_Segment|CustomerCount|\n+-----------+-------------+\n|      5-5-5|          309|\n|      1-1-1|          260|\n|      4-5-5|          161|\n|      1-2-2|          160|\n|      4-4-4|          120|\n|      2-1-1|          119|\n|      2-3-3|          111|\n|      2-2-2|          102|\n|      3-3-3|           99|\n|      3-4-4|           92|\n+-----------+-------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Register RFM DataFrame as a temporary SQL view\n",
    "rfm_df.createOrReplaceTempView(\"rfm_table\")\n",
    "\n",
    "# Example 1: Count of customers per RFM segment\n",
    "rfm_count = spark.sql(\"\"\"\n",
    "SELECT RFM_Segment, COUNT(*) AS CustomerCount\n",
    "FROM rfm_table\n",
    "GROUP BY RFM_Segment\n",
    "ORDER BY CustomerCount DESC\n",
    "\"\"\")\n",
    "rfm_count.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcfec3e3-7287-4bcc-bed7-9d1f09887ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "online-retail-rfm-pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}